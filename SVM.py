# -*- coding: utf-8 -*-
"""ML_HW04_MahdiSaieedi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tvCnvevgEYpmVvr4q5LvH1VtDxl3UFh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

from sklearn.datasets import load_svmlight_files
import urllib.request

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score
import cvxopt
from sklearn.svm import SVC

# Download dataset
url = "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/satimage.scale.tr"
urllib.request.urlretrieve(url, "satimage.scale.tr")

# Load dataset
X_train, y_train = load_svmlight_files(['satimage.scale.tr'])
print(X_train.shape)
print(y_train.shape)

# Download dataset
url2 = "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/satimage.scale.t"
urllib.request.urlretrieve(url2, "satimage.scale.t")

# Load dataset
X_test, y_test = load_svmlight_files(['satimage.scale.t'])
print(X_test.shape)
print(y_test.shape)

# Download dataset
url3 = "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/satimage.scale.val"
urllib.request.urlretrieve(url3, "satimage.scale.val")

# Load dataset
X_val, y_val = load_svmlight_files(['satimage.scale.val'])
print(X_val.shape)
print(y_val.shape)

type(X_train) , type(y_train)

X_train[0][0]

# Classes to keep
classes = [4, 6]

# Filter rows by y_train values
keep = np.where(np.isin(y_train, classes))[0]

# Apply indices to sparse X_train
X_filtered = X_train[keep, :]

# Apply indices to dense y_train
y_filtered = y_train[keep]
print(X_filtered.shape)
print(y_filtered.shape)

type(y_filtered)

type(X_filtered)

y_filtered[250:350]

y_binary = np.where(y_filtered == 4, -1, np.where(y_filtered == 6, 1, y_filtered))
y_binary[250:350]

y = y_binary
X = X_filtered
# Parameters
C = 1.0
X = X.toarray()

# Kernel matrix
K = X @ X.T

# Construct QP

P = cvxopt.matrix(np.outer(y, y) * K)
q = cvxopt.matrix(-np.ones((len(y), 1)))

#G = cvxopt.matrix(np.diag(np.ones(len(y)) * -1))
top = np.diag(np.ones(len(y)) * -1)
bot = np.identity(len(y))
G = cvxopt.matrix(np.vstack((top, bot)))


left = np.zeros(len(y))
right  = np.ones(len(y)) * C
h = cvxopt.matrix(np.hstack((left,right)))

A = cvxopt.matrix(y, (1,len(y)))
b = cvxopt.matrix(np.zeros(1))

print(P.size, q.size, G.size, h.size, A.size, b.size)

# Solve QP model
sol = cvxopt.solvers.qp(P, q, G, h, A, b)

# Lagrange multipliers
alphas = np.ravel(sol['x'])

# Extract support vectors
sv = alphas > 1e-5
ind = np.arange(len(alphas))[sv]

# Dual solution
alpha_sv = alphas[sv]
SV = X_filtered[sv]
y_sv = y[sv]

print("Alphas = ", alpha_sv)
print("Support vectors = ", SV)
print("y support vectors = ", y_sv)

len(y_sv) , alphas[ : 25]

# Classes to keep
classes = [4, 6]

# Filter rows by y_train values
keep = np.where(np.isin(y_test, classes))[0]

# Apply indices to sparse X_train
X_test_filtered = X_test[keep, :]
X_test_filtered_duplicate = X_test_filtered

# Apply indices to dense y_train
y_test_filtered = y_test[keep]
print(X_test_filtered.shape)
print(y_test_filtered.shape)

type(X_test_filtered)

X_test_filtered = X_test_filtered.toarray()

type(X_test_filtered)

y_test_filtered[100:150]

y_test_binary = np.where(y_test_filtered == 4, -1, np.where(y_test_filtered == 6, 1, y_test_filtered))
y_test_binary[250:350]

K[1,1]

b = 0
for n in range(len(alpha_sv)):
  b += y_sv[n]
  b -= sum(alpha_sv * y_sv * K[ind[n],sv])
  b /= len(alpha_sv)

w = np.zeros(len(X[0]))
for n in range(len(alpha_sv)):
  w += alpha_sv[n] * y_sv[n] * SV[n]

w

b

def svm_predict(X_test):
    return np.sign(w * X_test.T + b)

# Make predictions on test set
y_pred = svm_predict(X_test_filtered)

y_test_filtered.shape
type(y_test_filtered)

type(y_pred)

y_pred = np.squeeze(np.asarray(y_pred))

y_pred.shape

# Accuracy
def accuracy_(y_true, y_pred):
  correct = np.sum(y_true == y_pred)
  total = len(y_true)
  return correct / total

# Confusion matrix
def confusion_matrix_(y_true, y_pred):
  unique = np.unique(y_true)
  matrix = np.zeros((len(unique), len(unique)))
  for i, t in enumerate(y_true):
    j = np.argmax(y_pred[i])
    matrix[t][j] += 1
  return matrix

# Balanced accuracy
def balanced_accuracy_(y_true, y_pred):
  unique, counts = np.unique(y_true, return_counts=True)
  class_weights = counts / len(y_true)
  scores = [accuracy(y_true[y_true==label], y_pred[y_true==label]) for label in unique]
  return np.average(scores, weights=class_weights)

# Evaluate metrics
accuracy = accuracy_score(y_test_binary, y_pred)
print("Accuracy:", accuracy)

conf_matrix = confusion_matrix(y_test_binary, y_pred)
print("Confusion Matrix:\n", conf_matrix)

bal_accuracy = balanced_accuracy_score(y_test_binary, y_pred)
print("Balanced Accuracy:", bal_accuracy)

"""**Implementing the RBF Kernel**"""

classes = [4, 6]
keep = np.where(np.isin(y_val, classes))[0]
X_val_filtered = X_val[keep, :]
y_val_filtered = y_val[keep]
y_val_binary = np.where(y_val_filtered == 4, -1, np.where(y_val_filtered == 6, 1, y_val_filtered))
print(X_val_filtered.shape)
print(y_val_binary.shape)
type(X_val_filtered),type(y_val_binary)

def RBF(x , y, gamma=1):
    return np.exp(-gamma * np.linalg.norm(x - y, ord=2))

def train_svm(inpute_data, label, parameter , Kernel ):
  y = label
  X = inpute_data
  C = parameter
  X = X.toarray()
  # Kernel matrix
  K = X @ X.T
  P = cvxopt.matrix(np.outer(y, y) * K)
  q = cvxopt.matrix(-np.ones((len(y), 1)))
  top = np.diag(np.ones(len(y)) * -1)
  bot = np.identity(len(y))
  G = cvxopt.matrix(np.vstack((top, bot)))
  left = np.zeros(len(y))
  right  = np.ones(len(y)) * C
  h = cvxopt.matrix(np.hstack((left,right)))
  A = cvxopt.matrix(y, (1,len(y)))
  b = cvxopt.matrix(np.zeros(1))
  sol = cvxopt.solvers.qp(P, q, G, h, A, b)
  alphas = np.ravel(sol['x'])
  sv = alphas > 1e-5
  ind = np.arange(len(alphas))[sv]
  alpha_sv = alphas[sv]
  SV = X_filtered[sv]
  y_sv = y[sv]
  b = 0
  for n in range(len(alpha_sv)):
    b += y_sv[n]
    b -= sum(alpha_sv * y_sv * K[ind[n],sv])
    b /= len(alpha_sv)

  return alpha_sv, SV, y_sv, b

def svm_prediction_kernel(x, kernel, a, sv, sv_y, b , g):
  x = x.toarray()
  y_predict = np.zeros(len(x))
  for i in range(len(x)):
    s = 0
    for _a, _sv_y, _sv in zip(a, sv_y, sv):
      s += _a * _sv_y * kernel(x[i], _sv , gamma=g)
    y_predict[i] = s
  p = y_predict + b
  return np.sign(p)

gammas = [0.01, 0.1, 0.2 ,0.5, 1, 10, 100]
for _gamma in gammas:
  print(f'the gamma value is :  {_gamma}')
  a1, sv1, sv_y1, b1 = train_svm(X_filtered, y_binary , 1.0 , RBF)
  y_val_pred = svm_prediction_kernel(X_val_filtered, RBF, a1, sv1, sv_y1, b1 ,_gamma)
  y_val_pred = np.squeeze(np.asarray(y_val_pred))
  accuracy = accuracy_score(y_val_binary, y_val_pred)
  print("Accuracy:", accuracy)
  conf_matrix = confusion_matrix(y_val_binary, y_val_pred)
  print("Confusion Matrix:\n", conf_matrix)
  bal_accuracy = balanced_accuracy_score(y_val_binary, y_val_pred)
  print("Balanced Accuracy:", bal_accuracy)

gamma_ = 1
sigma = np.sqrt(1 / (2 * gamma_))
print("the best value for gamma is:\n", gamma_)
print("the best value for sigam is :\n", sigma)

b

y_test_pred_kernel = svm_prediction_kernel(X_test_filtered_duplicate, RBF, alpha_sv, SV, y_sv, b ,gamma_)
y_test_pred_kernel = np.squeeze(np.asarray(y_test_pred_kernel))
accuracy = accuracy_score(y_test_binary, y_test_pred_kernel)
print("Accuracy:", accuracy)
conf_matrix = confusion_matrix(y_test_binary, y_test_pred_kernel)
print("Confusion Matrix:\n", conf_matrix)
bal_accuracy = balanced_accuracy_score(y_test_binary, y_test_pred_kernel)
print("Balanced Accuracy:", bal_accuracy)

"""Unfortunately it did not improve

**SVM With Scikit Learn**
"""

acc_best , conf_mat_best , blance_acc_best = 0, 0, 0
C_R = np.logspace(-3, 6, 20)
gamma_R = np.logspace(-6, 3, 10)
for _ in C_R:
    for g1 in gamma_R:
        model = SVC(C=_, gamma=g1)
        model.fit(X_train, y_train)
        y_predict = model.predict(X_test)
        acc = balanced_accuracy_score(y_test, y_predict)
        conf_mat = confusion_matrix(y_test, y_predict)
        blance_acc = balanced_accuracy_score(y_test, y_predict)
        if blance_acc > blance_acc_best:
          acc_best = acc
          conf_mat_best = conf_mat
          blance_acc_best = blance_acc
          print('######################################')
          print('found a better balanced accuracy')
          print("the C value:", _)
          print("the gamma value:", g1)
          print("Accuracy:", acc_best)
          print("Confusion Matrix:\n", conf_mat_best)
          print("Balanced Accuracy:", blance_acc_best)
          print('######################################')

print("\n")
print('######################################')
print("Accuracy:", acc_best)
print("Confusion Matrix:\n", conf_mat_best)
print("Balanced Accuracy:", blance_acc_best)

"""**Multi Class Classifier for SVM**"""

print(X_train.shape)
print(y_train.shape)

# Classes to keep
classes = [1, 3, 6]
keep = np.where(np.isin(y_train, classes))[0]

# Apply indices to sparse X_train
X_filtered_multi = X_train[keep, :]
X_filtered_multi = X_filtered_multi.toarray()
# Apply indices to dense y_train
y_filtered_multi = y_train[keep]
print(X_filtered_multi.shape)
print(y_filtered_multi.shape)

print(X_test.shape)
print(y_test.shape)

classes = [1, 3, 6]

keep2 = np.where(np.isin(y_test, classes))[0]

# Apply indices to sparse X_train
X_test_filtered_multi = X_test[keep2, :]
X_test_filtered_multi = X_test_filtered_multi.toarray()
# Apply indices to dense y_train
y_test_filtered_multi = y_test[keep2]

print(X_test_filtered_multi.shape)
print(y_test_filtered_multi.shape)

n_classes = np.unique(y_test_filtered_multi).shape[0]
n_classes

import numpy as np

class SVM:
    def __init__(self, C=1.0):
        self.C = C
        self.W = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.W = np.zeros((n_classes, n_features))

        gram = self.kernel(X, X)
        print(gram)
        for c in range(n_classes):

            indices = np.where(y == c)[0]
            Xc = X[indices,:]
            yc = y[indices]
            dual_coef = self.solve_dual(gram[np.ix_(indices, indices)], yc)
            self.W[c,:] = np.dot(dual_coef, Xc)

    def predict(self, X):
        y_pred = []
        for x in X:
            scores = np.dot(self.W, x)

            y_pred.append(np.argmax(scores))
        return np.array(y_pred)

    def kernel(self, X1, X2):
        return np.dot(X1, X2.T)

    def solve_dual(self, K, y):
        n_samples = len(y)
        alpha = np.zeros(n_samples)
        steps = 100
        C = self.C

        for i in range(steps):
            h = np.dot(K, alpha)
            E = h - y

            mask = np.logical_and(alpha > 0, alpha < C)
            alpha[mask] -= (E[mask] / K.diagonal()[mask])

            dual_obj = alpha.T @ h - 0.5 * alpha.T @ K @ alpha - y.T @ alpha
            primal_obj = 0.5 * np.linalg.norm(self.W) ** 2 + self.C * np.sum(np.maximum(0, 1 - y * E))
            gap = primal_obj - dual_obj
            if gap < 1e-5:
                break

        return alpha

svm = SVM(C=1.0)
svm.fit(X_filtered_multi, y_filtered_multi)

y_pred = svm.predict(X_test_filtered_multi)

acc = np.mean(y_pred == y_filtered_multi)

acc

